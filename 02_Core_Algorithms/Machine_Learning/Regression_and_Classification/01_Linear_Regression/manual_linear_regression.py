# -*- coding: utf-8 -*-
"""
ã€å…¨èƒ½ç‰ˆã€‘çº¿æ€§å›å½’æ·±åº¦è§£æä¸æ‰‹å†™å®ç° (Linear Regression Master File)

åŠŸèƒ½åŒ…å«ï¼š
1. ä»é›¶æ‰‹å†™ LinearRegression ç±» (åŸºäºæ¢¯åº¦ä¸‹é™)ã€‚
2. åœºæ™¯ä¸€ï¼šå•å˜é‡å›å½’å¯è§†åŒ– (æ‹Ÿåˆç›´çº¿)ã€‚
3. åœºæ™¯äºŒï¼šå¤šå˜é‡å›å½’éªŒè¯ (3Då¹³é¢/å¤šç»´ç‰¹å¾)ã€‚
4. åœºæ™¯ä¸‰ï¼šå­¦ä¹ ç‡(Learning Rate)è¿‡å¤§çš„åæœæ¼”ç¤º (æ¢¯åº¦çˆ†ç‚¸)ã€‚
5. åœºæ™¯å››ï¼šä¸ Sklearn å·¥ä¸šç•Œåº“çš„ç»“æœå¯¹æ¯”ã€‚

ä½œè€…: PengBo (AI-Learning-Path)
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression as SklearnLR
from sklearn.metrics import mean_squared_error

# ==========================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ‰‹å†™çº¿æ€§å›å½’ç±» (æ ¸å¿ƒçŸ¥è¯†ç‚¹)
# ==========================================
class MyLinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        åˆå§‹åŒ–è¶…å‚æ•°
        :param learning_rate: å­¦ä¹ ç‡ (Alpha) - å†³å®šä¸‹å±±æ­¥å­çš„å¤§å°
        :param n_iterations: è¿­ä»£æ¬¡æ•° - å†³å®šä¸‹å±±èµ°å¤šå°‘æ­¥
        """
        self.lr = learning_rate
        self.n_iter = n_iterations
        self.weights = None
        self.bias = None
        self.loss_history = []  # è®°å½•æ¯æ¬¡è¿­ä»£çš„æŸå¤±ï¼Œç”¨æ¥ç”»å›¾è§‚å¯Ÿ

    def fit(self, X, y):
        """
        è®­ç»ƒæ¨¡å‹ (Training / Fitting)
        åŸç†ï¼šä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³• (Gradient Descent) ä¸æ–­æ›´æ–°æƒé‡å’Œåç½®
        """
        n_samples, n_features = X.shape
        
        # 1. åˆå§‹åŒ–å‚æ•° (é€šå¸¸åˆå§‹åŒ–ä¸º0æˆ–å°çš„éšæœºæ•°)
        self.weights = np.zeros((n_features, 1))
        self.bias = 0

        # 2. æ¢¯åº¦ä¸‹é™å¾ªç¯
        for i in range(self.n_iter):
            # --- å‰å‘ä¼ æ’­ (Forward Propagation) ---
            # å…¬å¼: y_pred = X Â· w + b
            y_pred = np.dot(X, self.weights) + self.bias

            # --- è®¡ç®—æŸå¤± (Loss Calculation - MSE) ---
            # å…¬å¼: J = (1/m) * Î£(y_pred - y)^2
            loss = (1 / n_samples) * np.sum((y_pred - y) ** 2)
            self.loss_history.append(loss)

            # --- åå‘ä¼ æ’­ (Backward Propagation) ---
            # è¿™é‡Œçš„æ•°å­¦æ¨å¯¼æ˜¯é¢è¯•å¸¸è€ƒç‚¹ï¼
            # å¯¹ w æ±‚å¯¼: dw = (2/m) * X.T Â· (y_pred - y)
            dw = (2 / n_samples) * np.dot(X.T, (y_pred - y))
            # å¯¹ b æ±‚å¯¼: db = (2/m) * Î£(y_pred - y)
            db = (2 / n_samples) * np.sum(y_pred - y)

            # --- å‚æ•°æ›´æ–° (Parameter Update) ---
            # w = w - lr * dw
            self.weights -= self.lr * dw
            self.bias -= self.lr * db
            
            # (å¯é€‰) æ‰“å°è®­ç»ƒè¿›åº¦
            if i % 100 == 0:
                # print(f"Iter {i}: Loss {loss:.4f}")
                pass

    def predict(self, X):
        """é¢„æµ‹æ–°æ•°æ®"""
        return np.dot(X, self.weights) + self.bias

# ==========================================
# è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆæ•°æ®
# ==========================================
def generate_data(n_samples=100, noise=10):
    """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: y = 3x + 4 + noise"""
    np.random.seed(42)
    X = 2 * np.random.rand(n_samples, 1) # ç‰¹å¾ X (0åˆ°2ä¹‹é—´)
    y = 4 + 3 * X + np.random.randn(n_samples, 1) * (noise/10) # çœŸå®å€¼ y
    return X, y

# ==========================================
# ä¸»ç¨‹åºï¼šå„ç§åœºæ™¯æ¼”ç¤º
# ==========================================
if __name__ == "__main__":
    print("\n" + "="*50)
    print("ğŸš€ åœºæ™¯ä¸€ï¼šå•å˜é‡å›å½’ (Simple Linear Regression)")
    print("ç›®æ ‡ï¼šæ‹Ÿåˆ y = 3x + 4")
    print("="*50)
    
    # 1. å‡†å¤‡æ•°æ®
    X, y = generate_data()
    
    # 2. è®­ç»ƒæˆ‘ä»¬æ‰‹å†™çš„æ¨¡å‹
    model = MyLinearRegression(learning_rate=0.1, n_iterations=500)
    model.fit(X, y)
    
    print(f"ã€çœŸå®å‚æ•°ã€‘ w: 3, b: 4")
    print(f"ã€è®­ç»ƒç»“æœã€‘ w: {model.weights[0][0]:.4f}, b: {model.bias:.4f}")
    
    # 3. å¯è§†åŒ–
    plt.figure(figsize=(12, 5))
    
    # å­å›¾1ï¼šæ‹Ÿåˆç›´çº¿
    plt.subplot(1, 2, 1)
    plt.scatter(X, y, color='blue', alpha=0.5, label='Data')
    plt.plot(X, model.predict(X), color='red', linewidth=2, label='Prediction')
    plt.title('Fit Result: y = {:.2f}x + {:.2f}'.format(model.weights[0][0], model.bias))
    plt.legend()
    
    # å­å›¾2ï¼šLossä¸‹é™æ›²çº¿ (å…³é”®ï¼é¢è¯•å¿…çœ‹)
    plt.subplot(1, 2, 2)
    plt.plot(model.loss_history)
    plt.title('Loss Curve (Training Process)')
    plt.xlabel('Iterations')
    plt.ylabel('MSE Loss')
    plt.show()
    
    print("\nâœ… å­¦åˆ°äº†ä»€ä¹ˆï¼š")
    print("1. æŸå¤±å‡½æ•°(Loss)éšç€è¿­ä»£æ¬¡æ•°å¢åŠ è€Œè¿…é€Ÿä¸‹é™ï¼Œæœ€åè¶‹äºå¹³ç¨³ï¼ˆæ”¶æ•›ï¼‰ã€‚")
    print("2. å³ä½¿æœ‰å™ªå£°å¹²æ‰°ï¼Œæ¢¯åº¦ä¸‹é™ä¹Ÿèƒ½æ‰¾åˆ°æ¥è¿‘çœŸå®çš„æƒé‡å’Œåç½®ã€‚")

    # ==========================================
    
    print("\n" + "="*50)
    print("ğŸš€ åœºæ™¯äºŒï¼šå¯¹æ¯” Sklearn (Verify with Industry Standard)")
    print("="*50)
    
    # 1. Sklearn è®­ç»ƒ
    sk_model = SklearnLR()
    sk_model.fit(X, y)
    
    print(f"ã€My Modelã€‘ w: {model.weights[0][0]:.4f}, b: {model.bias:.4f}")
    print(f"ã€Sklearn ã€‘ w: {sk_model.coef_[0][0]:.4f}, b: {sk_model.intercept_[0]:.4f}")
    
    # éªŒè¯è¯¯å·®
    mse_my = mean_squared_error(y, model.predict(X))
    mse_sk = mean_squared_error(y, sk_model.predict(X))
    print(f"ã€MSEå·®å¼‚ã€‘: {abs(mse_my - mse_sk):.6f}")
    
    if abs(mse_my - mse_sk) < 0.1:
        print("\nâœ… ç»“è®ºï¼šæˆ‘ä»¬æ‰‹å†™çš„ç®—æ³•ç²¾åº¦è¾¾åˆ°äº†å·¥ä¸šçº§åº“çš„æ°´å¹³ï¼")
    else:
        print("\nâš ï¸ ç»“è®ºï¼šè¿˜éœ€è¦è°ƒæ•´å­¦ä¹ ç‡æˆ–è¿­ä»£æ¬¡æ•°ã€‚")

    # ==========================================

    print("\n" + "="*50)
    print("ğŸš€ åœºæ™¯ä¸‰ï¼šå¤šå˜é‡å›å½’ (Multivariate Regression)")
    print("ç›®æ ‡ï¼šæ‹Ÿåˆ y = 2*x1 + 5*x2 + 10")
    print("="*50)
    
    # ç”Ÿæˆå¤šç»´æ•°æ® (100è¡Œ, 2åˆ—)
    X_multi = np.random.rand(100, 2)
    # çœŸå®å…¬å¼: y = 2*x1 + 5*x2 + 10
    w_true = np.array([[2], [5]])
    b_true = 10
    y_multi = np.dot(X_multi, w_true) + b_true + np.random.randn(100, 1) * 0.1
    
    # è®­ç»ƒ
    model_multi = MyLinearRegression(learning_rate=0.1, n_iterations=1000)
    model_multi.fit(X_multi, y_multi)
    
    print(f"ã€çœŸå®æƒé‡ã€‘: [2, 5], åç½®: 10")
    print(f"ã€é¢„æµ‹æƒé‡ã€‘: {model_multi.weights.flatten().round(2)}, åç½®: {model_multi.bias:.2f}")
    print("\nâœ… å­¦åˆ°äº†ä»€ä¹ˆï¼šçŸ©é˜µè¿ç®— (np.dot) è®©æˆ‘ä»¬ä¸éœ€è¦ä¿®æ”¹ä»£ç å°±èƒ½è‡ªåŠ¨æ”¯æŒä»»æ„å¤šä¸ªç‰¹å¾ï¼")

    # ==========================================

    print("\n" + "="*50)
    print("ğŸ’£ åœºæ™¯å››ï¼šåé¢æ•™æ - å­¦ä¹ ç‡è¿‡å¤§ (Learning Rate Explosion)")
    print("è®¾å®š learning_rate = 1.5 (æ­¥å­è·¨å¤ªå¤§äº†)")
    print("="*50)
    
    # æ•…æ„è®¾ç½®å¾ˆå¤§çš„å­¦ä¹ ç‡
    bad_model = MyLinearRegression(learning_rate=1.8, n_iterations=10)
    try:
        bad_model.fit(X, y)
        print("Loss historyå‰5æ­¥:", bad_model.loss_history[:5])
    except Exception as e:
        print(f"æŠ¥é”™äº†: {e}")
        
    print("\nâš ï¸ è§‚å¯Ÿç»“æœï¼šLoss ä¸é™åå‡ï¼Œç”šè‡³å˜æˆäº† inf (æ— ç©·å¤§) æˆ– nan (éæ•°å­—)ã€‚")
    print("âŒ æ•™è®­ï¼šæ¢¯åº¦ä¸‹é™æ—¶ï¼Œå¦‚æœå­¦ä¹ ç‡å¤ªå¤§ï¼Œä¼šè·¨è¿‡æœ€ä½ç‚¹ï¼Œå¯¼è‡´æ¨¡å‹'å‘æ•£'ï¼Œæ°¸è¿œæ— æ³•æ”¶æ•›ã€‚")

    print("\n" + "="*50)
    print("ğŸ‰ å…¨æµç¨‹æ¼”ç¤ºç»“æŸï¼è¯·å°†æ­¤æ–‡ä»¶æäº¤åˆ° Git ä¿å­˜ã€‚")
    print("="*50)